{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design considerations for extracting data from San Francisco civic issue tracking public API\n",
    "\n",
    "Data would have to be pulled from https://data.sfgov.org/ so it makes sense to run a job periodically as a batch process (it doesn't seem likely that the data is updated close to real-time).\n",
    "\n",
    "System should be able to load data incrementally if possible. It this case it is possible through provided API (https://dev.socrata.com/foundry/data.sfgov.org/ktji-gk7t). Initial data import would have to use the big file but then endpoint can be used with a date filter to get incremental update. Full import procedure should be repeatable and automated.\n",
    "\n",
    "If the ETL process is complicated enough it's good to divide the whole process in components. Each component should follow basic UNIX philosophy: be composable, do one thing well, expect output of component to be input to another. If one component fails system should still be in a stable state without incomplete data, duplicates or data corruption.\n",
    "\n",
    "Extending this pipeline to handle more data source would be an example of good use case for modular architecture. Extracting data could be done by different components or maybe the same one but running with different parameters. Same elements of data normalization and processing could be probably shared so this design approach would enable code reusing. \n",
    "\n",
    "If the data is very big in volume and/or if we want to use multiple workers to speed up the process, data should be partitioned. Files should be read in bulk for performance.\n",
    "\n",
    "If multiple components are involved there should some kind of work flow system controlling their execution and allowing defining task dependencies and conditions. Such system should also handle failures in some way: maybe just reporting it, maybe retrying, running rollbacks etc. This could achieved through systems with more programmatic approach Airflow, Luigi, Spring Batch, full-on ETL system like Pentaho or Clover or custom one, implemented using Spark, Kafka or similar technology allowing distributing and controlling work between workers. Of course it's also possible to implement everything completely  by hand. \n",
    "\n",
    "Save, preferably non-local storage system should be used to store raw data from the the endpoint and data from intermediate steps. This is especially important if we want to use many distributed workers. That can self-hosted file system or Amazon S3, GCS or similar solution.\n",
    "\n",
    "Then would be time for other parts E from ETL operations like:\n",
    "* joining with other data set\n",
    "* aggregation \n",
    "* adding calculated values\n",
    "* transposing or pivoting\n",
    "or similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
